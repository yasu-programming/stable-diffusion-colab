#!/usr/bin/env python3
"""
SDXL ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ»ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Google Colab ã§ã®SDXLé–¢é€£å•é¡Œã‚’è‡ªå‹•ä¿®æ­£

ä½¿ç”¨æ–¹æ³•:
    python scripts/sdxl_fix.py

æ©Ÿèƒ½:
- ä¸€èˆ¬çš„ãªå•é¡Œã®è‡ªå‹•è¨ºæ–­
- ãƒ¡ãƒ¢ãƒªé–¢é€£å•é¡Œã®ä¿®æ­£
- ä¾å­˜é–¢ä¿‚ã®ä¿®æ­£
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- æ®µéšçš„ä¿®å¾©æ‰‹é †
"""

import os
import sys
import subprocess
import time
import gc
from pathlib import Path

def print_header():
    """ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    print("=" * 60)
    print("ğŸ”§ SDXL ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° & ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print("ğŸ¯ å¯¾å¿œå¯èƒ½ãªå•é¡Œ:")
    print("   - CUDA Out of Memory ã‚¨ãƒ©ãƒ¼")
    print("   - ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
    print("   - ä¾å­˜é–¢ä¿‚ã®å•é¡Œ")
    print("   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹")
    print("   - ç”Ÿæˆå¤±æ•—")
    print("=" * 60)

def diagnose_system():
    """ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­"""
    print("\nğŸ” ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­ä¸­...")
    
    issues = []
    
    # Pythonãƒ»GPUç¢ºèª
    try:
        import torch
        print(f"   âœ… PyTorch: {torch.__version__}")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            allocated = torch.cuda.memory_allocated() / 1e9
            usage_percent = (allocated / gpu_memory) * 100
            
            print(f"   âœ… GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            print(f"   ğŸ“Š VRAMä½¿ç”¨: {allocated:.2f}GB / {gpu_memory:.1f}GB ({usage_percent:.1f}%)")
            
            if gpu_memory < 8:
                issues.append("low_vram")
                print("   âš ï¸  VRAMä¸è¶³: 8GBæœªæº€ã®GPU")
            
            if usage_percent > 80:
                issues.append("high_memory_usage")
                print("   âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é«˜: 80%ä»¥ä¸Š")
                
        else:
            issues.append("no_cuda")
            print("   âŒ CUDA: åˆ©ç”¨ä¸å¯")
            
    except ImportError:
        issues.append("no_pytorch")
        print("   âŒ PyTorch: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    # ä¾å­˜é–¢ä¿‚ç¢ºèª
    required_packages = {
        'diffusers': '0.19.0',
        'transformers': None,
        'safetensors': None,
        'accelerate': None,
        'invisible_watermark': None
    }
    
    for package, min_version in required_packages.items():
        try:
            pkg = __import__(package)
            version = getattr(pkg, '__version__', 'unknown')
            print(f"   âœ… {package}: {version}")
            
            if min_version and version < min_version:
                issues.append(f"old_{package}")
                print(f"   âš ï¸  {package}: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¤ã„ï¼ˆè¦æ±‚: {min_version}ä»¥ä¸Šï¼‰")
                
        except ImportError:
            issues.append(f"missing_{package}")
            print(f"   âŒ {package}: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    return issues

def fix_memory_issues():
    """ãƒ¡ãƒ¢ãƒªé–¢é€£å•é¡Œã®ä¿®æ­£"""
    print("\nğŸ§¹ ãƒ¡ãƒ¢ãƒªå•é¡Œä¿®æ­£ä¸­...")
    
    try:
        import torch
        
        # ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            print("   âœ… CUDA ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†")
        
        # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()
        print("   âœ… Python ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"   ğŸ“Š ã‚¯ãƒªã‚¢å¾ŒVRAM: {allocated:.2f}GB / {total:.1f}GB")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å¤±æ•—: {e}")
        return False

def fix_dependencies():
    """ä¾å­˜é–¢ä¿‚ã®ä¿®æ­£"""
    print("\nğŸ“¦ ä¾å­˜é–¢ä¿‚ä¿®æ­£ä¸­...")
    
    # é‡è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    critical_packages = [
        "diffusers>=0.19.0",
        "transformers", 
        "safetensors",
        "accelerate"
    ]
    
    success = True
    
    for package in critical_packages:
        print(f"   ğŸ”„ {package} ä¿®æ­£ä¸­...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pip", "install", "--upgrade", package],
                capture_output=True, text=True, timeout=300
            )
            
            if result.returncode == 0:
                print(f"   âœ… {package} ä¿®æ­£å®Œäº†")
            else:
                print(f"   âŒ {package} ä¿®æ­£å¤±æ•—")
                success = False
                
        except subprocess.TimeoutExpired:
            print(f"   â° {package} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            success = False
        except Exception as e:
            print(f"   âŒ {package} ã‚¨ãƒ©ãƒ¼: {e}")
            success = False
    
    return success

def fix_model_loading():
    """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å•é¡Œã®ä¿®æ­£"""
    print("\nğŸ¨ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¿®æ­£ä¸­...")
    
    try:
        # Hugging Face ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        cache_dir = Path.home() / ".cache" / "huggingface"
        if cache_dir.exists():
            print("   ğŸ—‘ï¸  Hugging Face ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ä¸­...")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã¯æ…é‡ã«ï¼ˆå¿…è¦ãªéƒ¨åˆ†ã®ã¿ï¼‰
            transformers_cache = cache_dir / "transformers"
            if transformers_cache.exists():
                import shutil
                shutil.rmtree(transformers_cache)
                print("   âœ… Transformers ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        print("   ğŸ§ª ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆä¸­...")
        
        import torch
        from diffusers import DiffusionPipeline
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ã‚ˆã‚Šä¿å®ˆçš„ãªè¨­å®šã§ãƒ†ã‚¹ãƒˆ
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
            low_cpu_mem_usage=True
        )
        
        # æœ€å°é™ã®æœ€é©åŒ–
        pipe.enable_model_cpu_offload()
        
        print("   âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"   âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def optimize_performance():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"""
    print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ä¸­...")
    
    optimizations = []
    
    try:
        import torch
        from diffusers import DiffusionPipeline
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãƒ†ã‚¹ãƒˆ
        print("   ğŸ§ª æœ€é©åŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¸­...")
        
        # ä¸€æ™‚çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16"
        )
        
        # åŸºæœ¬æœ€é©åŒ–
        pipe.enable_model_cpu_offload()
        optimizations.append("CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰")
        
        pipe.vae.enable_tiling()
        optimizations.append("VAE ã‚¿ã‚¤ãƒªãƒ³ã‚°")
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        try:
            pipe.enable_xformers_memory_efficient_attention()
            optimizations.append("xFormers")
        except:
            print("   âš ï¸  xFormers: åˆ©ç”¨ä¸å¯")
        
        try:
            pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
            optimizations.append("torch.compile")
        except:
            print("   âš ï¸  torch.compile: åˆ©ç”¨ä¸å¯")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print(f"   âœ… åˆ©ç”¨å¯èƒ½ãªæœ€é©åŒ–: {', '.join(optimizations)}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æœ€é©åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def create_optimized_config():
    """æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ"""
    print("\nğŸ“ æœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­...")
    
    try:
        config = {
            "model_settings": {
                "torch_dtype": "float16",
                "use_safetensors": True,
                "variant": "fp16",
                "low_cpu_mem_usage": True
            },
            "optimization_settings": {
                "enable_model_cpu_offload": True,
                "enable_vae_tiling": True,
                "enable_xformers": "auto",
                "enable_torch_compile": "auto"
            },
            "generation_settings": {
                "youtube_thumbnail": {
                    "height": 576,
                    "width": 1024,
                    "num_inference_steps": 25,
                    "guidance_scale": 8.0
                },
                "blog_image": {
                    "height": 536,
                    "width": 1024,
                    "num_inference_steps": 30,
                    "guidance_scale": 7.5
                },
                "icon": {
                    "height": 1024,
                    "width": 1024,
                    "num_inference_steps": 35,
                    "guidance_scale": 9.0
                }
            },
            "memory_settings": {
                "max_batch_size": 1,
                "cleanup_interval": 5,
                "enable_attention_slicing": True
            }
        }
        
        import json
        with open("sdxl_optimized_config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print("   âœ… æœ€é©åŒ–è¨­å®šä¿å­˜: sdxl_optimized_config.json")
        return True
        
    except Exception as e:
        print(f"   âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {e}")
        return False

def run_recovery_test():
    """ä¿®å¾©ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ä¿®å¾©ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    
    try:
        import torch
        from diffusers import DiffusionPipeline
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("   ğŸ”„ SDXL ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–...")
        
        # ä¿®å¾©ã•ã‚ŒãŸè¨­å®šã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
            low_cpu_mem_usage=True
        )
        
        # ä¿®å¾©ã•ã‚ŒãŸæœ€é©åŒ–ã‚’é©ç”¨
        pipe.enable_model_cpu_offload()
        pipe.vae.enable_tiling()
        
        print("   ğŸ¨ ãƒ†ã‚¹ãƒˆç”»åƒç”Ÿæˆä¸­...")
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆç”Ÿæˆ
        test_image = pipe(
            prompt="A simple test image, digital art",
            height=512,  # å°ã•ã‚ã®è§£åƒåº¦ã§ãƒ†ã‚¹ãƒˆ
            width=512,
            num_inference_steps=15,  # çŸ­æ™‚é–“ã§ãƒ†ã‚¹ãƒˆ
            guidance_scale=7.0,
            num_images_per_prompt=1
        ).images[0]
        
        # ä¿å­˜
        os.makedirs("generated_images", exist_ok=True)
        test_path = "generated_images/recovery_test.png"
        test_image.save(test_path)
        
        print(f"   âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_path}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del pipe, test_image
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"   âŒ ä¿®å¾©ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def print_recommendations(issues):
    """æ¨å¥¨äº‹é …ã®è¡¨ç¤º"""
    print("\nğŸ’¡ æ¨å¥¨äº‹é …ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:")
    
    if "low_vram" in issues:
        print("   ğŸ”¸ VRAMä¸è¶³å¯¾ç­–:")
        print("      - è§£åƒåº¦ã‚’ä¸‹ã’ã‚‹ (768x768 ä»¥ä¸‹)")
        print("      - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«åˆ¶é™")
        print("      - enable_sequential_cpu_offload() ã‚’ä½¿ç”¨")
    
    if "high_memory_usage" in issues:
        print("   ğŸ”¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›:")
        print("      - å®šæœŸçš„ã« torch.cuda.empty_cache() å®Ÿè¡Œ")
        print("      - ä¸è¦ãªå¤‰æ•°ã‚’ del ã§å‰Šé™¤")
        print("      - gc.collect() ã§ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    
    if any("missing_" in issue for issue in issues):
        print("   ğŸ”¸ ä¾å­˜é–¢ä¿‚ä¿®æ­£:")
        print("      - python setup.py ã‚’å†å®Ÿè¡Œ")
        print("      - pip install --upgrade ã‚’å€‹åˆ¥å®Ÿè¡Œ")
    
    print("\n   ğŸ”¸ ä¸€èˆ¬çš„ãªãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:")
    print("      - ç”Ÿæˆå‰å¾Œã§ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢")
    print("      - é©åˆ‡ãªè§£åƒåº¦è¨­å®š")
    print("      - æ®µéšçš„ãªã‚¹ãƒ†ãƒƒãƒ—æ•°èª¿æ•´")
    print("      - å®šæœŸçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢")

def main():
    """ãƒ¡ã‚¤ãƒ³ä¿®å¾©å‡¦ç†"""
    start_time = time.time()
    
    try:
        print_header()
        
        # ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­
        issues = diagnose_system()
        
        if not issues:
            print("\nâœ… å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return True
        
        print(f"\nğŸ”§ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(issues)}ä»¶")
        for issue in issues:
            print(f"   - {issue}")
        
        # ä¿®å¾©å‡¦ç†å®Ÿè¡Œ
        repairs_attempted = 0
        repairs_successful = 0
        
        # ãƒ¡ãƒ¢ãƒªå•é¡Œä¿®æ­£
        if any("memory" in issue for issue in issues):
            repairs_attempted += 1
            if fix_memory_issues():
                repairs_successful += 1
        
        # ä¾å­˜é–¢ä¿‚ä¿®æ­£
        if any("missing_" in issue or "old_" in issue for issue in issues):
            repairs_attempted += 1
            if fix_dependencies():
                repairs_successful += 1
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¿®æ­£
        repairs_attempted += 1
        if fix_model_loading():
            repairs_successful += 1
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        repairs_attempted += 1
        if optimize_performance():
            repairs_successful += 1
        
        # æœ€é©åŒ–è¨­å®šä½œæˆ
        create_optimized_config()
        
        # ä¿®å¾©ãƒ†ã‚¹ãƒˆ
        print("\nğŸ ä¿®å¾©å®Œäº†ãƒ†ã‚¹ãƒˆ...")
        test_success = run_recovery_test()
        
        # çµæœè¡¨ç¤º
        total_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ ä¿®å¾©çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        print(f"ğŸ”§ ä¿®å¾©è©¦è¡Œ: {repairs_attempted}")
        print(f"âœ… ä¿®å¾©æˆåŠŸ: {repairs_successful}")
        print(f"ğŸ§ª æœ€çµ‚ãƒ†ã‚¹ãƒˆ: {'æˆåŠŸ' if test_success else 'å¤±æ•—'}")
        print(f"â±ï¸  ä¿®å¾©æ™‚é–“: {total_time:.1f}ç§’")
        
        if test_success:
            print("\nğŸ‰ ä¿®å¾©ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print("   é€šå¸¸ã®ç”»åƒç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚")
        else:
            print("\nâš ï¸  ä¸€éƒ¨ã®å•é¡ŒãŒè§£æ±ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            print("   æ‰‹å‹•ã§ã®è¨­å®šèª¿æ•´ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        
        # æ¨å¥¨äº‹é …è¡¨ç¤º
        print_recommendations(issues)
        
        print("=" * 60)
        
        return test_success
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ä¿®å¾©ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ ä¿®å¾©ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)