#!/usr/bin/env python3
"""
SDXL ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Google Colab ç’°å¢ƒã§ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’æœ€å¤§åŒ–

ä½¿ç”¨æ–¹æ³•:
    python scripts/memory_optimizer.py

æ©Ÿèƒ½:
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°åˆ†æ
- ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- æœ€é©åŒ–è¨­å®šã®è‡ªå‹•èª¿æ•´
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªç”»åƒç”Ÿæˆè¨­å®š
"""

import gc
import os
import sys
import time
import psutil
from pathlib import Path

def print_header():
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    print("=" * 60)
    print("ğŸ§  SDXL ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print("ğŸ¯ æœ€é©åŒ–å†…å®¹:")
    print("   - GPUãƒ»RAMãƒ¡ãƒ¢ãƒªã®è©³ç´°åˆ†æ")
    print("   - ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    print("   - æœ€é©åŒ–è¨­å®šã®è‡ªå‹•èª¿æ•´")
    print("   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    print("=" * 60)

def analyze_memory_usage():
    """è©³ç´°ãƒ¡ãƒ¢ãƒªåˆ†æ"""
    print("\nğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†æä¸­...")
    
    memory_info = {}
    
    # RAMä½¿ç”¨é‡
    ram = psutil.virtual_memory()
    memory_info['ram'] = {
        'total_gb': ram.total / 1e9,
        'used_gb': ram.used / 1e9,
        'available_gb': ram.available / 1e9,
        'usage_percent': ram.percent
    }
    
    print(f"   ğŸ’¾ RAM:")
    print(f"      ç·å®¹é‡: {memory_info['ram']['total_gb']:.1f}GB")
    print(f"      ä½¿ç”¨ä¸­: {memory_info['ram']['used_gb']:.1f}GB")
    print(f"      åˆ©ç”¨å¯èƒ½: {memory_info['ram']['available_gb']:.1f}GB")
    print(f"      ä½¿ç”¨ç‡: {memory_info['ram']['usage_percent']:.1f}%")
    
    # GPU ãƒ¡ãƒ¢ãƒª
    try:
        import torch
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated()
            reserved = torch.cuda.memory_reserved()
            total = torch.cuda.get_device_properties(0).total_memory
            
            memory_info['gpu'] = {
                'total_gb': total / 1e9,
                'allocated_gb': allocated / 1e9,
                'reserved_gb': reserved / 1e9,
                'free_gb': (total - reserved) / 1e9,
                'usage_percent': (allocated / total) * 100
            }
            
            print(f"   ğŸ® GPU VRAM:")
            print(f"      ç·å®¹é‡: {memory_info['gpu']['total_gb']:.1f}GB")
            print(f"      ä½¿ç”¨ä¸­: {memory_info['gpu']['allocated_gb']:.2f}GB")
            print(f"      äºˆç´„æ¸ˆã¿: {memory_info['gpu']['reserved_gb']:.2f}GB")
            print(f"      ç©ºã: {memory_info['gpu']['free_gb']:.2f}GB")
            print(f"      ä½¿ç”¨ç‡: {memory_info['gpu']['usage_percent']:.1f}%")
        else:
            print("   âŒ GPU VRAM: CUDAåˆ©ç”¨ä¸å¯")
            memory_info['gpu'] = None
    except ImportError:
        print("   âŒ GPU VRAM: PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        memory_info['gpu'] = None
    
    return memory_info

def aggressive_memory_cleanup():
    """ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    print("\nğŸ§¹ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
    
    cleanup_results = {}
    
    # PyTorch GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
    try:
        import torch
        if torch.cuda.is_available():
            before_allocated = torch.cuda.memory_allocated() / 1e9
            before_reserved = torch.cuda.memory_reserved() / 1e9
            
            # å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            
            # è¤‡æ•°å›å®Ÿè¡Œã—ã¦ç¢ºå®Ÿã«ã‚¯ãƒªã‚¢
            for _ in range(3):
                torch.cuda.empty_cache()
                time.sleep(0.1)
            
            after_allocated = torch.cuda.memory_allocated() / 1e9
            after_reserved = torch.cuda.memory_reserved() / 1e9
            
            cleanup_results['gpu'] = {
                'before_allocated': before_allocated,
                'after_allocated': after_allocated,
                'before_reserved': before_reserved,
                'after_reserved': after_reserved,
                'freed_allocated': before_allocated - after_allocated,
                'freed_reserved': before_reserved - after_reserved
            }
            
            print(f"   ğŸ® GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢:")
            print(f"      è§£æ”¾ (ä½¿ç”¨ä¸­): {cleanup_results['gpu']['freed_allocated']:.2f}GB")
            print(f"      è§£æ”¾ (äºˆç´„æ¸ˆã¿): {cleanup_results['gpu']['freed_reserved']:.2f}GB")
            
    except Exception as e:
        print(f"   âŒ GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å¤±æ•—: {e}")
    
    # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    before_objects = len(gc.get_objects())
    collected = gc.collect()
    after_objects = len(gc.get_objects())
    
    cleanup_results['python'] = {
        'objects_before': before_objects,
        'objects_after': after_objects,
        'objects_collected': collected,
        'objects_reduced': before_objects - after_objects
    }
    
    print(f"   ğŸ Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³:")
    print(f"      å›åã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {cleanup_results['python']['objects_collected']}")
    print(f"      å‰Šæ¸›ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {cleanup_results['python']['objects_reduced']}")
    
    # ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèªï¼ˆå‚è€ƒæƒ…å ±ï¼‰
    try:
        # /proc/meminfoãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼ˆLinuxï¼‰
        if os.path.exists('/proc/meminfo'):
            with open('/proc/meminfo', 'r') as f:
                meminfo = f.read()
                for line in meminfo.split('\n'):
                    if 'Cached:' in line:
                        cached_kb = int(line.split()[1])
                        cached_gb = cached_kb / 1e6
                        print(f"   ğŸ’¾ ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cached_gb:.2f}GB")
                        break
    except:
        pass
    
    return cleanup_results

def optimize_pipeline_settings():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–è¨­å®š"""
    print("\nâš™ï¸ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–è¨­å®šç”Ÿæˆä¸­...")
    
    try:
        # ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã«åŸºã¥ãè¨­å®š
        memory_info = analyze_memory_usage()
        
        # GPU VRAM ã«åŸºã¥ãè¨­å®šèª¿æ•´
        if memory_info['gpu']:
            vram_gb = memory_info['gpu']['total_gb']
            
            if vram_gb >= 12:
                tier = "high"
                print("   ğŸš€ é«˜æ€§èƒ½è¨­å®š (12GBä»¥ä¸Š)")
            elif vram_gb >= 8:
                tier = "medium"
                print("   âš¡ æ¨™æº–è¨­å®š (8-12GB)")
            else:
                tier = "low"
                print("   ğŸ”‹ çœãƒ¡ãƒ¢ãƒªè¨­å®š (8GBæœªæº€)")
        else:
            tier = "low"
            print("   ğŸ”‹ CPUè¨­å®š (CUDAç„¡åŠ¹)")
        
        # è¨­å®šå®šç¾©
        settings = {
            "high": {
                "model_settings": {
                    "torch_dtype": "float16",
                    "use_safetensors": True,
                    "variant": "fp16"
                },
                "optimization": {
                    "enable_model_cpu_offload": False,
                    "enable_sequential_cpu_offload": False,
                    "enable_vae_tiling": True,
                    "enable_attention_slicing": False
                },
                "generation_limits": {
                    "max_height": 1536,
                    "max_width": 1536,
                    "max_batch_size": 2,
                    "max_inference_steps": 50
                }
            },
            "medium": {
                "model_settings": {
                    "torch_dtype": "float16",
                    "use_safetensors": True,
                    "variant": "fp16"
                },
                "optimization": {
                    "enable_model_cpu_offload": True,
                    "enable_sequential_cpu_offload": False,
                    "enable_vae_tiling": True,
                    "enable_attention_slicing": True
                },
                "generation_limits": {
                    "max_height": 1024,
                    "max_width": 1024,
                    "max_batch_size": 1,
                    "max_inference_steps": 40
                }
            },
            "low": {
                "model_settings": {
                    "torch_dtype": "float16",
                    "use_safetensors": True,
                    "variant": "fp16"
                },
                "optimization": {
                    "enable_model_cpu_offload": True,
                    "enable_sequential_cpu_offload": True,
                    "enable_vae_tiling": True,
                    "enable_attention_slicing": True
                },
                "generation_limits": {
                    "max_height": 768,
                    "max_width": 768,
                    "max_batch_size": 1,
                    "max_inference_steps": 30
                }
            }
        }
        
        selected_settings = settings[tier]
        
        # ç”¨é€”åˆ¥æ¨å¥¨è¨­å®š
        use_case_settings = {
            "youtube_thumbnail": {
                "height": min(576, selected_settings["generation_limits"]["max_height"]),
                "width": min(1024, selected_settings["generation_limits"]["max_width"]),
                "num_inference_steps": min(25, selected_settings["generation_limits"]["max_inference_steps"]),
                "guidance_scale": 8.0
            },
            "blog_image": {
                "height": min(536, selected_settings["generation_limits"]["max_height"]),
                "width": min(1024, selected_settings["generation_limits"]["max_width"]),
                "num_inference_steps": min(30, selected_settings["generation_limits"]["max_inference_steps"]),
                "guidance_scale": 7.5
            },
            "icon": {
                "height": min(1024, selected_settings["generation_limits"]["max_height"]),
                "width": min(1024, selected_settings["generation_limits"]["max_width"]),
                "num_inference_steps": min(35, selected_settings["generation_limits"]["max_inference_steps"]),
                "guidance_scale": 9.0
            }
        }
        
        # è¨­å®šä¿å­˜
        import json
        config = {
            "memory_tier": tier,
            "vram_gb": memory_info['gpu']['total_gb'] if memory_info['gpu'] else 0,
            "pipeline_settings": selected_settings,
            "use_case_settings": use_case_settings,
            "memory_management": {
                "cleanup_interval": 5,
                "enable_gc_collect": True,
                "enable_torch_cuda_empty_cache": True
            }
        }
        
        with open("memory_optimized_config.json", "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… æœ€é©åŒ–è¨­å®šä¿å­˜: memory_optimized_config.json")
        print(f"   ğŸ“Š è¨­å®šãƒ¬ãƒ™ãƒ«: {tier}")
        
        return config
        
    except Exception as e:
        print(f"   âŒ è¨­å®šç”Ÿæˆå¤±æ•—: {e}")
        return None

def create_memory_monitor():
    """ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ"""
    print("\nğŸ“ˆ ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆä¸­...")
    
    monitor_script = '''
import torch
import gc
import time

class MemoryMonitor:
    def __init__(self):
        self.peak_gpu_memory = 0
        self.peak_ram_usage = 0
        
    def check_memory(self, label=""):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            self.peak_gpu_memory = max(self.peak_gpu_memory, allocated)
            print(f"[{label}] GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved (peak: {self.peak_gpu_memory:.2f}GB)")
        
        import psutil
        ram_usage = psutil.virtual_memory().percent
        self.peak_ram_usage = max(self.peak_ram_usage, ram_usage)
        print(f"[{label}] RAM: {ram_usage:.1f}% (peak: {self.peak_ram_usage:.1f}%)")
    
    def cleanup_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("Memory cleanup completed")

# ä½¿ç”¨ä¾‹:
# monitor = MemoryMonitor()
# monitor.check_memory("Before generation")
# # ... ç”»åƒç”Ÿæˆå‡¦ç† ...
# monitor.check_memory("After generation")
# monitor.cleanup_memory()
'''
    
    try:
        with open("memory_monitor.py", "w", encoding="utf-8") as f:
            f.write(monitor_script)
        print("   âœ… ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ: memory_monitor.py")
        return True
    except Exception as e:
        print(f"   âŒ ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå¤±æ•—: {e}")
        return False

def test_optimized_generation():
    """æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã§ã®ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
    print("\nğŸ§ª æœ€é©åŒ–è¨­å®šãƒ†ã‚¹ãƒˆä¸­...")
    
    try:
        import torch
        from diffusers import DiffusionPipeline
        import json
        
        # æœ€é©åŒ–è¨­å®šèª­ã¿è¾¼ã¿
        if os.path.exists("memory_optimized_config.json"):
            with open("memory_optimized_config.json", "r", encoding="utf-8") as f:
                config = json.load(f)
        else:
            print("   âš ï¸  æœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("   ğŸ”„ æœ€é©åŒ–è¨­å®šã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–...")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
        pipe_settings = config["pipeline_settings"]["model_settings"]
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            **pipe_settings
        )
        
        # æœ€é©åŒ–é©ç”¨
        opt_settings = config["pipeline_settings"]["optimization"]
        
        if opt_settings["enable_model_cpu_offload"]:
            pipe.enable_model_cpu_offload()
            print("   âœ… CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æœ‰åŠ¹")
        
        if opt_settings["enable_sequential_cpu_offload"]:
            pipe.enable_sequential_cpu_offload()
            print("   âœ… ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æœ‰åŠ¹")
        
        if opt_settings["enable_vae_tiling"]:
            pipe.vae.enable_tiling()
            print("   âœ… VAE ã‚¿ã‚¤ãƒªãƒ³ã‚°æœ‰åŠ¹")
        
        if opt_settings["enable_attention_slicing"]:
            pipe.enable_attention_slicing()
            print("   âœ… Attention ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°æœ‰åŠ¹")
        
        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        if torch.cuda.is_available():
            before_memory = torch.cuda.memory_allocated() / 1e9
            print(f"   ğŸ“Š ç”Ÿæˆå‰GPUä½¿ç”¨é‡: {before_memory:.2f}GB")
        
        # ãƒ†ã‚¹ãƒˆç”Ÿæˆï¼ˆæœ€å°é™ã®è¨­å®šï¼‰
        test_settings = config["use_case_settings"]["youtube_thumbnail"]
        test_settings["height"] = min(512, test_settings["height"])  # ã•ã‚‰ã«å°ã•ã
        test_settings["width"] = min(512, test_settings["width"])
        test_settings["num_inference_steps"] = min(15, test_settings["num_inference_steps"])
        
        print("   ğŸ¨ ãƒ†ã‚¹ãƒˆç”»åƒç”Ÿæˆä¸­...")
        start_time = time.time()
        
        test_image = pipe(
            prompt="Simple test image, digital art",
            **test_settings,
            num_images_per_prompt=1
        ).images[0]
        
        generation_time = time.time() - start_time
        
        # ãƒ¡ãƒ¢ãƒªç›£è¦–çµ‚äº†
        if torch.cuda.is_available():
            after_memory = torch.cuda.memory_allocated() / 1e9
            print(f"   ğŸ“Š ç”Ÿæˆå¾ŒGPUä½¿ç”¨é‡: {after_memory:.2f}GB")
            print(f"   ğŸ“Š GPUä½¿ç”¨é‡å¢—åŠ : {after_memory - before_memory:.2f}GB")
        
        # ä¿å­˜
        os.makedirs("generated_images", exist_ok=True)
        test_path = "generated_images/memory_optimized_test.png"
        test_image.save(test_path)
        
        print(f"   âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
        print(f"   â±ï¸  ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
        print(f"   ğŸ“¸ ä¿å­˜å…ˆ: {test_path}")
        print(f"   ğŸ“ è§£åƒåº¦: {test_image.size}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del pipe, test_image
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"   âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

def print_memory_tips():
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ã‚³ãƒ„"""
    print("\nğŸ’¡ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ã‚³ãƒ„:")
    print("   ğŸ”¸ ç”Ÿæˆå‰å¾Œã®ç¿’æ…£:")
    print("      - torch.cuda.empty_cache() ã§GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢")
    print("      - gc.collect() ã§Pythonãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢") 
    print("      - ä¸è¦ãªå¤‰æ•°ã¯ del ã§å‰Šé™¤")
    
    print("\n   ğŸ”¸ ç”Ÿæˆè¨­å®šã®èª¿æ•´:")
    print("      - è§£åƒåº¦ã‚’æ®µéšçš„ã«ä¸‹ã’ã‚‹")
    print("      - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«åˆ¶é™")
    print("      - æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å‰Šæ¸›")
    
    print("\n   ğŸ”¸ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–:")
    print("      - enable_model_cpu_offload() ä½¿ç”¨")
    print("      - enable_vae_tiling() ã§é«˜è§£åƒåº¦å¯¾å¿œ")
    print("      - enable_attention_slicing() ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„")
    
    print("\n   ğŸ”¸ ç·Šæ€¥æ™‚ã®å¯¾å‡¦:")
    print("      - Colabãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•")
    print("      - ã‚ˆã‚Šå°ã•ã„è§£åƒåº¦ã§å†è©¦è¡Œ")
    print("      - enable_sequential_cpu_offload() ä½¿ç”¨")

def main():
    """ãƒ¡ã‚¤ãƒ³æœ€é©åŒ–å‡¦ç†"""
    start_time = time.time()
    
    try:
        print_header()
        
        # ãƒ¡ãƒ¢ãƒªåˆ†æ
        memory_before = analyze_memory_usage()
        
        # ç©æ¥µçš„ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleanup_results = aggressive_memory_cleanup()
        
        # æœ€é©åŒ–å¾Œã®ãƒ¡ãƒ¢ãƒªçŠ¶æ³
        print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ãƒ¡ãƒ¢ãƒªçŠ¶æ³:")
        memory_after = analyze_memory_usage()
        
        # æœ€é©åŒ–è¨­å®šç”Ÿæˆ
        config = optimize_pipeline_settings()
        
        # ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
        create_memory_monitor()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_success = test_optimized_generation()
        
        # çµæœã‚µãƒãƒªãƒ¼
        total_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–çµæœ")
        print("=" * 60)
        
        if memory_before['gpu'] and memory_after['gpu']:
            gpu_improvement = memory_before['gpu']['usage_percent'] - memory_after['gpu']['usage_percent']
            print(f"ğŸ® GPU ãƒ¡ãƒ¢ãƒªæ”¹å–„: {gpu_improvement:.1f}%")
        
        ram_improvement = memory_before['ram']['usage_percent'] - memory_after['ram']['usage_percent']
        print(f"ğŸ’¾ RAM æ”¹å–„: {ram_improvement:.1f}%")
        
        print(f"ğŸ§ª æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ: {'æˆåŠŸ' if test_success else 'å¤±æ•—'}")
        print(f"â±ï¸  æœ€é©åŒ–æ™‚é–“: {total_time:.1f}ç§’")
        
        if test_success:
            print("\nğŸ‰ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            print("   memory_optimized_config.json ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        else:
            print("\nâš ï¸  ä¸€éƒ¨ã®æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸ")
        
        # ã‚³ãƒ„ã®è¡¨ç¤º
        print_memory_tips()
        
        print("=" * 60)
        
        return test_success
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æœ€é©åŒ–ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ æœ€é©åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)