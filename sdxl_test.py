#!/usr/bin/env python3
"""
SDXL Base 1.0 è©³ç´°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†å¾Œã®å‹•ä½œç¢ºèªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python sdxl_test.py

æ©Ÿèƒ½:
- SDXL ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‹•ä½œç¢ºèª
- å„ç”¨é€”åˆ¥ç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
- å“è³ªè©•ä¾¡
"""

import os
import sys
import time
import json
import gc
from datetime import datetime
from pathlib import Path

def print_header():
    """ãƒ†ã‚¹ãƒˆé–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    print("=" * 60)
    print("ğŸ§ª SDXL Base 1.0 è©³ç´°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print("ğŸ“‹ ãƒ†ã‚¹ãƒˆå†…å®¹:")
    print("   - ç’°å¢ƒãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¢ºèª")
    print("   - YouTube ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("   - Noteãƒ»ãƒ–ãƒ­ã‚°ç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("   - ã‚¢ã‚¤ã‚³ãƒ³ãƒ»ãƒ­ã‚´ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š")
    print("   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯")
    print("=" * 60)

def check_setup():
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª"""
    print("\nğŸ” ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªä¸­...")
    
    setup_checks = {
        'torch': False,
        'diffusers': False,
        'transformers': False,
        'cuda': False,
        'sdxl_model': False
    }
    
    try:
        import torch
        setup_checks['torch'] = True
        print(f"   âœ… PyTorch: {torch.__version__}")
        
        if torch.cuda.is_available():
            setup_checks['cuda'] = True
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"   âœ… CUDA: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("   âŒ CUDA: åˆ©ç”¨ä¸å¯")
            
    except ImportError:
        print("   âŒ PyTorch: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    try:
        import diffusers
        setup_checks['diffusers'] = True
        print(f"   âœ… Diffusers: {diffusers.__version__}")
    except ImportError:
        print("   âŒ Diffusers: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    try:
        import transformers
        setup_checks['transformers'] = True
        print(f"   âœ… Transformers: {transformers.__version__}")
    except ImportError:
        print("   âŒ Transformers: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
    required_dirs = ['generated_images', 'logs']
    for directory in required_dirs:
        if Path(directory).exists():
            print(f"   âœ… Directory: {directory}/")
        else:
            Path(directory).mkdir(exist_ok=True)
            print(f"   ğŸ“ Created: {directory}/")
    
    # åŸºæœ¬çš„ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    basic_setup_ok = all([
        setup_checks['torch'],
        setup_checks['diffusers'], 
        setup_checks['transformers'],
        setup_checks['cuda']
    ])
    
    if basic_setup_ok:
        print("âœ… åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªå®Œäº†")
    else:
        print("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚setup.py ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return False
    
    return True

def initialize_pipeline():
    """SDXL ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–"""
    print("\nğŸ¨ SDXL ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
    
    try:
        import torch
        from diffusers import DiffusionPipeline
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("   ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        
        # SDXL ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16"
        )
        
        load_time = time.time() - start_time
        print(f"   âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.2f}ç§’)")
        
        # æœ€é©åŒ–é©ç”¨
        print("   âš¡ æœ€é©åŒ–é©ç”¨ä¸­...")
        pipe.enable_model_cpu_offload()
        pipe.vae.enable_tiling()
        
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³æœ€é©åŒ–
        optimizations = []
        try:
            pipe.enable_xformers_memory_efficient_attention()
            optimizations.append("xFormers")
        except:
            pass
        
        try:
            pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
            optimizations.append("torch.compile")
        except:
            pass
        
        if optimizations:
            print(f"   âœ… æœ€é©åŒ–æœ‰åŠ¹: {', '.join(optimizations)}")
        else:
            print("   âš ï¸  è¿½åŠ æœ€é©åŒ–ãªã—")
        
        print("âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        return pipe
        
    except Exception as e:
        print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
        return None

def test_youtube_generation(pipe):
    """YouTube ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¬ YouTube ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    
    test_prompts = [
        {
            "name": "gaming",
            "prompt": "Excited gamer with headphones playing video game, colorful RGB background, high energy",
            "style": "gaming"
        },
        {
            "name": "tech_tutorial", 
            "prompt": "Professional programmer at clean desk with multiple monitors, coding tutorial",
            "style": "educational"
        }
    ]
    
    results = []
    
    for test in test_prompts:
        print(f"\n   ğŸ“ ãƒ†ã‚¹ãƒˆ: {test['name']}")
        print(f"      ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test['prompt'][:50]}...")
        
        try:
            start_time = time.time()
            
            images = pipe(
                prompt=f"{test['prompt']}, YouTube thumbnail style, professional photography, ultra detailed",
                negative_prompt="blurry, low quality, boring, text overlay",
                height=576,  # 16:9
                width=1024,
                num_inference_steps=25,
                guidance_scale=8.0,
                num_images_per_prompt=1
            ).images
            
            generation_time = time.time() - start_time
            
            # ä¿å­˜
            filename = f"generated_images/test_youtube_{test['name']}.png"
            images[0].save(filename)
            
            print(f"      âœ… æˆåŠŸ ({generation_time:.2f}ç§’) - {filename}")
            
            results.append({
                'type': 'youtube',
                'name': test['name'],
                'success': True,
                'time': generation_time,
                'file': filename,
                'size': images[0].size
            })
            
        except Exception as e:
            print(f"      âŒ å¤±æ•—: {e}")
            results.append({
                'type': 'youtube',
                'name': test['name'],
                'success': False,
                'error': str(e)
            })
    
    return results

def test_blog_generation(pipe):
    """ãƒ–ãƒ­ã‚°ç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“ ãƒ–ãƒ­ã‚°ç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    
    test_prompts = [
        {
            "name": "tech_workspace",
            "prompt": "Modern laptop on clean desk with coffee cup and notebook, minimalist workspace",
            "category": "tech"
        },
        {
            "name": "lifestyle_cozy",
            "prompt": "Cozy living room with warm lighting, books and plants, comfortable atmosphere",
            "category": "lifestyle"
        }
    ]
    
    results = []
    
    for test in test_prompts:
        print(f"\n   ğŸ“ ãƒ†ã‚¹ãƒˆ: {test['name']}")
        print(f"      ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test['prompt'][:50]}...")
        
        try:
            start_time = time.time()
            
            images = pipe(
                prompt=f"{test['prompt']}, blog header image, professional photography, high quality",
                negative_prompt="cluttered, messy, unprofessional, low quality",
                height=536,  # 1.91:1
                width=1024,
                num_inference_steps=30,
                guidance_scale=7.5,
                num_images_per_prompt=1
            ).images
            
            generation_time = time.time() - start_time
            
            # ä¿å­˜
            filename = f"generated_images/test_blog_{test['name']}.png"
            images[0].save(filename)
            
            print(f"      âœ… æˆåŠŸ ({generation_time:.2f}ç§’) - {filename}")
            
            results.append({
                'type': 'blog',
                'name': test['name'],
                'success': True,
                'time': generation_time,
                'file': filename,
                'size': images[0].size
            })
            
        except Exception as e:
            print(f"      âŒ å¤±æ•—: {e}")
            results.append({
                'type': 'blog',
                'name': test['name'],
                'success': False,
                'error': str(e)
            })
    
    return results

def test_icon_generation(pipe):
    """ã‚¢ã‚¤ã‚³ãƒ³ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ¨ ã‚¢ã‚¤ã‚³ãƒ³ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    
    test_prompts = [
        {
            "name": "tech_minimal",
            "prompt": "Technology innovation symbol with geometric shapes, minimalist design",
            "style": "minimalist"
        },
        {
            "name": "creative_modern",
            "prompt": "Creative arts symbol with modern design elements",
            "style": "modern"
        }
    ]
    
    results = []
    
    for test in test_prompts:
        print(f"\n   ğŸ“ ãƒ†ã‚¹ãƒˆ: {test['name']}")
        print(f"      ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test['prompt'][:50]}...")
        
        try:
            start_time = time.time()
            
            images = pipe(
                prompt=f"{test['prompt']}, logo icon, scalable vector style, professional branding",
                negative_prompt="complex, detailed, photorealistic, cluttered, text, letters",
                height=1024,  # 1:1
                width=1024,
                num_inference_steps=35,
                guidance_scale=9.0,
                num_images_per_prompt=1
            ).images
            
            generation_time = time.time() - start_time
            
            # ä¿å­˜
            filename = f"generated_images/test_icon_{test['name']}.png"
            images[0].save(filename)
            
            print(f"      âœ… æˆåŠŸ ({generation_time:.2f}ç§’) - {filename}")
            
            results.append({
                'type': 'icon',
                'name': test['name'],
                'success': True,
                'time': generation_time,
                'file': filename,
                'size': images[0].size
            })
            
        except Exception as e:
            print(f"      âŒ å¤±æ•—: {e}")
            results.append({
                'type': 'icon',
                'name': test['name'],
                'success': False,
                'error': str(e)
            })
    
    return results

def performance_benchmark(pipe):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
    print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š...")
    
    benchmark_configs = [
        {"name": "å¿«é€Ÿ", "steps": 15, "size": (768, 768)},
        {"name": "æ¨™æº–", "steps": 25, "size": (1024, 1024)},
        {"name": "é«˜å“è³ª", "steps": 40, "size": (1024, 1024)}
    ]
    
    benchmark_results = []
    test_prompt = "Beautiful mountain landscape, digital art, high quality"
    
    for config in benchmark_configs:
        print(f"\n   â±ï¸  {config['name']}è¨­å®šãƒ†ã‚¹ãƒˆ...")
        print(f"      è§£åƒåº¦: {config['size'][0]}Ã—{config['size'][1]}")
        print(f"      ã‚¹ãƒ†ãƒƒãƒ—æ•°: {config['steps']}")
        
        times = []
        
        # 3å›æ¸¬å®šã—ã¦å¹³å‡ã‚’å–ã‚‹
        for i in range(3):
            try:
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                if hasattr(pipe, 'cuda'):
                    torch.cuda.empty_cache()
                gc.collect()
                
                start_time = time.time()
                
                images = pipe(
                    prompt=test_prompt,
                    height=config['size'][1],
                    width=config['size'][0],
                    num_inference_steps=config['steps'],
                    guidance_scale=7.5,
                    num_images_per_prompt=1
                ).images
                
                generation_time = time.time() - start_time
                times.append(generation_time)
                
                print(f"      è©¦è¡Œ {i+1}: {generation_time:.2f}ç§’")
                
            except Exception as e:
                print(f"      è©¦è¡Œ {i+1}: å¤±æ•— - {e}")
                break
        
        if times:
            avg_time = sum(times) / len(times)
            min_time = min(times)
            max_time = max(times)
            
            print(f"      ğŸ“Š å¹³å‡: {avg_time:.2f}ç§’ (æœ€é€Ÿ: {min_time:.2f}s, æœ€é…: {max_time:.2f}s)")
            
            benchmark_results.append({
                'config': config['name'],
                'avg_time': avg_time,
                'min_time': min_time,
                'max_time': max_time,
                'successful_runs': len(times)
            })
        else:
            print(f"      âŒ ã™ã¹ã¦ã®è©¦è¡ŒãŒå¤±æ•—")
    
    return benchmark_results

def check_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯...")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            total = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            usage_percent = (allocated / total) * 100
            
            print(f"   GPU ãƒ¡ãƒ¢ãƒª:")
            print(f"      ä½¿ç”¨ä¸­: {allocated:.2f}GB")
            print(f"      äºˆç´„æ¸ˆã¿: {reserved:.2f}GB")
            print(f"      ç·å®¹é‡: {total:.2f}GB")
            print(f"      ä½¿ç”¨ç‡: {usage_percent:.1f}%")
            
            if usage_percent > 80:
                print("   âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„ã§ã™")
            elif usage_percent > 60:
                print("   ğŸ˜ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ™®é€šã§ã™")
            else:
                print("   âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯è‰¯å¥½ã§ã™")
            
            return {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'total_gb': total,
                'usage_percent': usage_percent
            }
        else:
            print("   âŒ CUDA GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
            
    except Exception as e:
        print(f"   âŒ ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")
        return None

def save_test_results(test_data):
    """ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜"""
    print("\nğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœä¿å­˜ä¸­...")
    
    try:
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"sdxl_test_results_{timestamp}.json"
        
        # çµæœã‚’JSONã§ä¿å­˜
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"   âœ… ãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {log_file}")
        return log_file
        
    except Exception as e:
        print(f"   âŒ çµæœä¿å­˜å¤±æ•—: {e}")
        return None

def print_summary(all_results):
    """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    # å…¨ä½“çµ±è¨ˆ
    total_tests = 0
    successful_tests = 0
    total_time = 0
    
    for category, results in all_results.items():
        if category == 'generation_tests':
            for result in results:
                total_tests += 1
                if result['success']:
                    successful_tests += 1
                    total_time += result.get('time', 0)
    
    success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
    avg_time = total_time / successful_tests if successful_tests > 0 else 0
    
    print(f"ğŸ¯ å…¨ä½“çµæœ:")
    print(f"   ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ•°: {total_tests}")
    print(f"   æˆåŠŸ: {successful_tests}")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"   å¹³å‡ç”Ÿæˆæ™‚é–“: {avg_time:.2f}ç§’")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
    print(f"\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ:")
    categories = {}
    for result in all_results.get('generation_tests', []):
        cat = result['type']
        if cat not in categories:
            categories[cat] = {'total': 0, 'success': 0, 'avg_time': 0}
        
        categories[cat]['total'] += 1
        if result['success']:
            categories[cat]['success'] += 1
            categories[cat]['avg_time'] += result.get('time', 0)
    
    for cat, stats in categories.items():
        success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
        avg_time = stats['avg_time'] / stats['success'] if stats['success'] > 0 else 0
        print(f"   {cat.title()}: {stats['success']}/{stats['total']} ({success_rate:.0f}%) - {avg_time:.1f}så¹³å‡")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ
    if 'benchmark' in all_results:
        print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        for bench in all_results['benchmark']:
            print(f"   {bench['config']}: {bench['avg_time']:.2f}ç§’å¹³å‡")
    
    # ãƒ¡ãƒ¢ãƒªçŠ¶æ³
    if 'memory' in all_results and all_results['memory']:
        memory = all_results['memory']
        print(f"\nğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory['usage_percent']:.1f}% ({memory['allocated_gb']:.2f}GB/{memory['total_gb']:.1f}GB)")
    
    # æ¨å¥¨äº‹é …
    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
    if success_rate < 80:
        print("   - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„")
    if avg_time > 120:
        print("   - ã‚ˆã‚Šé«˜é€Ÿãªè¨­å®šã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
    if all_results.get('memory', {}).get('usage_percent', 0) > 80:
        print("   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¦ãã ã•ã„")
    
    print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸç”»åƒ:")
    print("   generated_images/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    print("=" * 60)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    start_time = time.time()
    
    try:
        print_header()
        
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
        if not check_setup():
            return False
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        pipe = initialize_pipeline()
        if pipe is None:
            return False
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        all_results = {
            'test_start_time': datetime.now().isoformat(),
            'generation_tests': [],
            'benchmark': [],
            'memory': None
        }
        
        # å„ç¨®ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        youtube_results = test_youtube_generation(pipe)
        blog_results = test_blog_generation(pipe)
        icon_results = test_icon_generation(pipe)
        
        all_results['generation_tests'].extend(youtube_results)
        all_results['generation_tests'].extend(blog_results)
        all_results['generation_tests'].extend(icon_results)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        benchmark_results = performance_benchmark(pipe)
        all_results['benchmark'] = benchmark_results
        
        # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        memory_info = check_memory_usage()
        all_results['memory'] = memory_info
        
        # çµæœä¿å­˜
        total_time = time.time() - start_time
        all_results['total_test_time'] = total_time
        all_results['test_end_time'] = datetime.now().isoformat()
        
        save_test_results(all_results)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_summary(all_results)
        
        print(f"\nâ±ï¸  ç·ãƒ†ã‚¹ãƒˆæ™‚é–“: {total_time/60:.1f}åˆ†")
        print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        return True
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return False
    except Exception as e:
        print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)